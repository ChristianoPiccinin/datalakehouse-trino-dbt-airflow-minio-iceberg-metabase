name: datalake-platform

services:
  # ===================== TRINO =====================
  trino:
    image: trinodb/trino:${TRINO_VERSION}
    ports:
      - "8082:8080"                        # evita conflito com Airflow; ajuste se quiser
    environment:
      - TRINO_PASSWORD_FILE=/etc/trino/password.db
    depends_on:
      hive-metastore:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./trino/etc/trino/catalog:/etc/trino/catalog
      - ./trino/etc/trino/password.db:/etc/trino/password.db:ro
    networks:
      - datalake
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/v1/info >/dev/null"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 30s

  # ===================== AIRFLOW DB (Postgres) =====================
  airflow-db:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
      POSTGRES_DB: airflow_db
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    networks:
      - datalake
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow_db -h localhost"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

  # ===================== AIRFLOW =====================
  airflow-init:
    image: ${AIRFLOW_IMAGE}
    depends_on:
      airflow-db:
        condition: service_healthy
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${AIRFLOW_DB_PASSWORD}@airflow-db:5432/airflow_db
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_WWW_USER_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_WWW_USER_PASSWORD}
      _AIRFLOW_WWW_USER_EMAIL: ${AIRFLOW_WWW_USER_EMAIL}
      _AIRFLOW_WWW_USER_FIRSTNAME: Admin
      _AIRFLOW_WWW_USER_LASTNAME: User
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    entrypoint: /bin/bash
    command: -lc "
      airflow db migrate &&
      airflow users create \
        --username ${AIRFLOW_WWW_USER_USERNAME} \
        --password ${AIRFLOW_WWW_USER_PASSWORD} \
        --firstname ${_AIRFLOW_WWW_USER_FIRSTNAME:-Admin} \
        --lastname ${_AIRFLOW_WWW_USER_LASTNAME:-User} \
        --role Admin \
        --email ${AIRFLOW_WWW_USER_EMAIL} || true
    "
    networks:
      - datalake
    restart: "no"

  airflow-webserver:
    image: ${AIRFLOW_IMAGE}
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8000:8080"
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${AIRFLOW_DB_PASSWORD}@airflow-db:5432/airflow_db
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    command: webserver
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    networks:
      - datalake
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/health | grep -q 'healthy'"]
      interval: 20s
      timeout: 5s
      retries: 20
      start_period: 40s

  airflow-scheduler:
    image: ${AIRFLOW_IMAGE}
    depends_on:
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${AIRFLOW_DB_PASSWORD}@airflow-db:5432/airflow_db
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    command: scheduler
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    networks:
      - datalake
    restart: unless-stopped

  # ===================== MINIO =====================
  minio:
    image: minio/minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"
      - "9001:9001"
    command: ["server", "/data", "--console-address", ":9001"]
    volumes:
      - minio_data:/data
    networks:
      - datalake
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9000/minio/health/live >/dev/null"]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 20s

  # ===================== HIVE METASTORE DB (Postgres 11) =====================
  metastore-db:
    image: postgres:11
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: ${HIVE_DB_PASSWORD}
      POSTGRES_DB: metastore
    command: ["postgres", "-c", "wal_level=logical"]
    volumes:
      - metastore_db_data:/var/lib/postgresql/data
    networks:
      - datalake
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore -h localhost"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

  # ===================== HIVE METASTORE (Thrift) =====================
  hive-metastore:
    image: starburstdata/hive:3.1.2-e.18
    depends_on:
      metastore-db:
        condition: service_healthy
      minio:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: jdbc:postgresql://metastore-db:5432/metastore
      HIVE_METASTORE_USER: hive
      HIVE_METASTORE_PASSWORD: ${HIVE_DB_PASSWORD}
      HIVE_METASTORE_WAREHOUSE_DIR: s3://warehouse/
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: ${MINIO_ROOT_USER}
      S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      S3_PATH_STYLE_ACCESS: "true"
      REGION: ""
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin"
    networks:
      - datalake
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'exec 6<>/dev/tcp/localhost/9083'"]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 20s

  # ===================== METABASE DB (Postgres) =====================
  metabase-db:
    image: postgres:13
    environment:
      POSTGRES_DB: metabase
      POSTGRES_USER: metabase
      POSTGRES_PASSWORD: ${METABASE_DB_PASSWORD}
    volumes:
      - metabase_db_data:/var/lib/postgresql/data
    networks:
      - datalake
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U metabase -d metabase -h localhost"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

  # ===================== METABASE =====================
  metabase:
    image: metabase/metabase:${METABASE_VERSION}
    depends_on:
      metabase-db:
        condition: service_healthy
      trino:
        condition: service_healthy
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabase
      MB_DB_PORT: 5432
      MB_DB_USER: metabase
      MB_DB_PASS: ${METABASE_DB_PASSWORD}
      MB_DB_HOST: metabase-db
      # (Dica) conexão ao Trino é feita via UI; essas vars abaixo são opcionais:
      MB_TRINO_HOST: trino
      MB_TRINO_PORT: 8080
      MB_TRINO_USER: trino
      MB_TRINO_PASS: trino
    volumes:
      - metabase_data:/metabase-data
    networks:
      - datalake
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:3000/api/health >/dev/null"]
      interval: 20s
      timeout: 5s
      retries: 15
      start_period: 40s

# ===================== VOLUMES & NETWORKS =====================
volumes:
  airflow_db_data:
  airflow_logs:
  airflow_plugins:
  metastore_db_data:
  metabase_db_data:
  metabase_data:
  minio_data:

networks:
  datalake:
    name: trino-iceberg-hive-network
